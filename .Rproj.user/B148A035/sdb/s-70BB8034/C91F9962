{
    "contents" : "---\ntitle: \"Computational Statistics Exam ST522\"\nauthor: \"Emil H. Andersen & Nguyen Khanh le Ho\"\ndate: \"4 June 2016\"\noutput: pdf_document\n---\n\nTo reproduce the calculations done in this project, it is required to install the following package.\n\n```{r Package_Installer}\ndevtools::install_github(\"Chaiji/LemilExamST522\")\nlibrary(LemilExamST522)\n```\n\n#Exam Part A.\n\n##Task 1.\n\nTo solve the questions in this section, we use the data from the link:\n\n<https://raw.githubusercontent.com/haghish/ST516/master/data/sport.txt>\n\n```{r Read_Sport}\n#Read data from the link\nsport <- read.table(\"https://raw.githubusercontent.com/haghish/ST516/master/data/sport.txt\")\n```\n\n###Task 1.1\n\n_The parameter of interest in task is the correlation between the time spent daily for physical activity and the semester average grade._\n\nFor this task, let $T$ denote the time spent daily for physical activity in hour, and let $G$ denote the semester average grade. Thus our parameter of interest is \n\\[ \\text{corr}(T,G) = \\frac{\\text{cov}(T,G)}{\\sigma_T \\sigma_G} = \\frac{\\text{E}[(T-\\mu_T)(G-\\mu_G)]}{\\sigma_T \\sigma_G}\\]\n\nTo estimate the correlation of $T$ and $G$, take a sample of size $n$, $(T_i,G_i)_{i =1,...,n}$ and calculate the sample correlation \n\\[ \\hat{\\theta} = \\frac{\\sum\\limits_{i=1}^n (T_i-\\bar{T}) (G_i - \\bar{G})}{\\sqrt{\\sum\\limits_{i=1}^n (T_i - \\bar{T})^2 \\sum\\limits_{i=1}^n (G_i - \\bar{G})^2}} = \\frac{S_{TG}}{S_TS_G}\\]\n\nWhere $S_{TG}$, $S_T$ and $S_G$ are the sample covariance, standard deviations of $T$ and $G$ respectively. This can be calculated in R using the function $\\text{cor}$.\n\n```{r Correlation}\n#Print and assign value simultaneously\nprint(correlation <- cor(sport$Sport,sport$Grades))\n```\n\n###Task 1.2\n\n_We want to use bootstrap to estimate correlation, bias, standard error and a $95\\%$ confidence interval for the variables of interest. All without using bootstrapping packages._\n\nWe choose to do the bootstrap with $2000$ bootstrap samples. The correlation of each sample is calculated. The mean of all the correlations is the estimated bootstrap correlation.\n\n```{r Bootstrapping}\n#We set a seed before first random action.\nset.seed(516)\nn <- 2000\nN <- nrow(sport)\nstorage <- numeric(n)\n#Bootstrapping\nfor (i in 1:n){\n  j <- sample(1:N, size = N, replace = TRUE)\n  Sports <- sport$Sport[j]\n  Grades <- sport$Grades[j]\n  storage[i] <- cor(Sports,Grades)\n}\nprint(estCor <- mean(storage))\n```\n\n**Bias**\n\nTo estimate the bias of an estimator ( in our case the sample correlation), the correlation is defined as the expected difference between the estimator $\\hat{\\theta}$ and the true value $\\theta$ which it is trying to estimate i.e.\n\\[\n\\text{bias}(\\theta)= E(\\hat{\\theta} - \\theta) = E(\\hat{\\theta})- \\theta\n\\]\n\nTo obtain this value we simply estimate $E(\\hat{\\theta})$ by averaging over the bootstrap estimates of the correlation, and $\\theta$ is replaced by the correlation calculated directly from our data. That is if $\\hat{\\theta}$ is the estimation obtained directly from our sample and $\\hat{\\theta}_1 , ... , \\hat{\\theta}_N$ are the bootstrap replicates of $\\hat{\\theta}$, then\n\\[\n\\widehat{\\text{bias}}(\\theta) = \\frac{1}{N} \\sum\\limits_{i=1}^N \\hat{\\theta}_i -\\hat{\\theta}\n\\]\nThis is justified as both $\\frac{1}{N} \\sum\\limits_{i=1}^N \\hat{\\theta}_i$ and $\\hat{\\theta}$ are supposed to be close to the true value they estimate.\n\nBased on the bootstrap samples generated earlier we estimate the bias as follows\n\n```{r Bias}\nprint(bias <- mean(storage) - correlation)\n```\n\nIdeally, we want the bias to be close to zero, since a bias measures the error to be expected when estimating some parameter based on a sample. In other words, it represents how accurate our estimation of the underlying parameter is.\n\n**Standard Error**\n\nThe standard error is the standard deviation of an estimator i.e.\n\\[\\text{SE}_{\\hat{\\theta}} = \\text{sd}(\\hat{\\theta})\\]\nUsing the bootstrap replicates we can obtain this by \n\\[\\text{SE}_{\\hat{\\theta}} = S = \\sqrt{\\frac{1}{N-1}\\sum\\limits_{i=1}^N ( \\hat{\\theta}_i - \\bar{ \\hat{\\theta}} )^2}\\]\n\nThis can be calculated based on the bootstrap replicates above as follows\n\n```{r Standard_Error}\nprint(SE <- sd(storage)) #Standard Error\n```\n\nLike the bias, the standard error is also a measure of accuracy of an estimator in the sense that it gives us an idea on how much our estimates are spread out. Moreover, the standard error is used to estimate the confidence interval.\n\n**Confidence Interval**\n\nWe are interested in the $95\\%$ confidence interval. That is an interval where the true value of the parameter lies within with a probability of $95\\%$. In general, a $1-\\alpha$ confidence interval is given by\n\\[\n(\\bar{\\hat{\\theta}}- z_{\\alpha /2}SE,\\bar{\\hat{\\theta}}+ z_{\\alpha /2}SE)\n\\]\n\nWhere SE is the standard error, and $z_{\\alpha /2} = \\Phi^{-1}(1-{\\alpha/2})$, such that $P(Z>z_{\\alpha /2})$, with $Z \\sim N(0,1)$. This interval is derived from the fact given by the Central Limit Theorem, that\n\\[\n\\frac{\\sqrt{n}(\\bar{\\hat{\\theta}} - \\theta)}{\\sigma_\\theta}\\sim N(0,1)\n\\]\n\\[\\approx \\frac{(\\bar{\\hat{\\theta}} - \\theta)}{SE}\\]\n\nHence\n\\[P\\left(  - z_{\\alpha /2} <\\frac{(\\bar{\\hat{\\theta}} - \\theta)}{SE} < z_{\\alpha /2}\\right) = P \\left(  \\bar{\\hat{\\theta}} - z_{\\alpha /2} SE <\\theta < \\bar{\\hat{\\theta}}+z_{\\alpha /2} SE \\right)= 1-\\alpha \\]\n\nIn our case $\\alpha = 0.05$ which yields $z_{\\alpha /2} = \\Phi^{-1}(1-{0.025}) = 1.96$. With this we can estimate the confidence interval as follows\n\n```{r CI}\nalpha <- 0.05\nval <- round(qnorm(1-alpha/2),digits=2) #1.96\n\nprint(CI <- c(estCor - val*SE, estCor + val*SE))\n```\n\nThus, with a probability of $95\\%$ the true value of our parameter of interest (the correlation) lies within the interval above.\n\n```{r Bootstrap_Histogram}\nhist(storage, main = \"Histogram of the bootstrap estimated correlation\", xlab = \"Correlation\")\nabline(v = c(CI[1],CI[2],correlation), col=c(\"blue\",\"blue\",\"red\"), lty=c(5,5,1))\n```\n\nThe blue dashed lines indicate our $95\\%$ confidence interval, and the red line indicates the estimated correlation from our data.\n\n###Task 1.3\n\nIn this task we created the function \"bootstrap.correlation(n=200,x,y,plot=FALSE)\" in the package \"LemilExamST522\". We added the ability to plot a histogram with the function by using \"plot=TRUE\". Please see the function for more information, by either opening it directly or reading the help file.(Note that the \"View()\" function hides comments in the code)\n\n##Task 2.\n\nIn this task we work with Buffon's Needle problem and Monte Carlo.\n\n###Task 2.1\n\nWe want to estimate $P(hit)$ using Monte Carlo on the following integral:\n$$P(hit)={\\int_0^{\\pi} l sin(\\theta)d \\theta \\over \\pi d}$$\nWe let $l = d = 1$.\n\nThe Monte Carlo approach is to generate uniform random numbers and estimate an integral by the average of the results of these random numbers. The formula is given by\n$$\\int_a^b f(x) dx = (b-a) \\int_0^1 f((b-a)x+a)dx$$\nIn this case, we have\n$$(\\pi-0) \\int_0^1 f((\\pi-0)x+0)dx = \\pi \\int_0^1 sin(\\pi \\cdot x) dx$$\nSo we get\n$$P(hit) = {\\pi \\int_0^1 sin(\\pi \\cdot x) dx \\over \\pi} = \\int_0^1 sin(\\pi \\cdot x) dx$$\n\nNote integral for $P(hit)$ with $l=d=1$ can also be expressed as the expected value of $Y=sin \\theta$, where $\\theta \\sim U(0,\\pi)$, that is\n\\[\nP(\\text{hit})=E(Y)= \\int_{0}^\\pi  Y \\frac{1}{\\pi } d\\theta\n\\]\nThus, to estimate this integral, we simply generate $X_1,...,X_n \\sim U(0,\\pi)$, then take the average of $sin(X_1),...,sin(X_n)$.\n\n```{r Monte_Carlo_2_1}\nn <- 100000\ntheta <- runif(n,0,pi)\nY <- sin(theta)\nprint(Est <- mean(Y))\n```\n\nThe variance, standard deviation and standard error of our estimator is of interest. We see that the variance of our estimator(In this case $\\bar{Y}$) is\n\\[\n\\text{var}(\\bar{Y}) =  \\frac{\\text{var}(Y)}{n}\n\\]\nIt follows that the standard deviation and standard error is\n\\[\nSE = \\text{sd}(\\bar{Y}) = \\sqrt{\\frac{\\text{var}(Y)}{n}} \n\\]\nUsing these results we estimate the variance and the standard error as well as the standard deviation.\n\n```{r 2_1_Monte_Carlo}\nVarEst <- var(Y)/n #Variance\nSE <- sqrt(VarEst) #Standard Error\nlist(\"Variance\" = VarEst, \"Standard_Error\" = SE)\n```\n\nThus, with $95\\%$ confidence the true value of this integral($2 \\over \\pi$) is within the interval\n\n```{r CI_Monte_Carlo_2_1}\nc(Est - 1.96*SE,Est + 1.96*SE)\n```\n\n###Task 2.2\n\nIn this task, we want to simulate Buffon's experiment. To do this, we have created the function \"buffon.needle(n=10,l=1,d=1)\" in the package \"LemilExamST522\". This function estimates $\\pi$ by simulating Buffon's experiment with n needles of length l with d spacing between the stripes. Please see the function for more information, either by opening the file directly, or reading the help file.\n\nNow, we wish to simulate this experiment for $n=10000$, with $l=d=1$.\n\n```{r 2_2_sim}\nprint(estimate <- buffon.needle(n=10000,l=1,d=1))\n```\n\n###Task 2.3\n\nWe wish to repeat this experiment for $n=1,10,...,10000$, to observe the changes of our estimates.\n\n```{r Simulation_1_10000}\nx <- c(1,seq(10,10000,10))\ny <- numeric(length(x))\ny[1] <- buffon.needle(1,1,1)\nfor(i in 1:1000){\n  y[i+1] <- buffon.needle(10*i,1,1)\n}\nplot(x,y,type=\"l\", ylab=\"Estimated Pi\", xlab=\"Number of experiments\")\nabline(h=pi, col=\"red\")\n```\n\nThe red line indcates the true value of $\\pi$. We observe that as the number of needles increase, we get a better approximation for $\\pi$. But note that while our estimator does seem to converge toward $\\pi$, there are still a lot of fluctuation even at the larger value for $n$. It seems that if one wants a very accurate approximation of $\\pi$, one would need to do this experiment with a huge amount of needles.\n\n###Task 2.4\n\n_We wish to replicate Lazzarini's experiment in this task._\n\n####Task 2.4a\n\n_We will replicate Lazzarini's experiment 10000 times, that means, running the simulation with $n=3408$, $l=2.5$ and $d=3$ 10000 times. Then we wish to compare the results with his estimate of 6 decimals._\n\nTo observe the results, we calculate the mean, standard error and confidence interval of the results, and use a scatterplot.\n\n```{r Lazzarini_10000}\nEstLa <- c()\nfor(i in 1:10000){\n  EstLa[i] <- buffon.needle(n = 3408, l= 2.5 , d = 3)  \n}\nEstLa <- round(EstLa,digits = 6) # rounding every estimates by 6 digits\nmEst <- mean(EstLa)              # The mean \nSE <- sd(EstLa)                  # Standard error\nCI <- c(mEst - 1.96*SE, mEst + 1.96*SE)                   # 95% conf. interval\nplot(EstLa, xlab = \"Experiment\", ylab = \"Estimated Pi\")\nabline(h = CI, col=c(\"red\",\"red\"), lty= 5)\nabline(h = 3.141593, col=\"blue\")\n```\n\nAbove is the result of 10000 replicates of Lazzarini's experiment. The red dashed lines indicates the $95\\%$ confidence interval. The blue line indicates the point $3.141593$ which he claimed is the result of his experiment. The scatter plot above shows that there are about $1.3\\%$ of the 10000 experiments that resulted in $3.141593$.\n\n```{r Estimate_length_Lazzarini_pi}\n#Number of estimates resulting in 6 digits of pi\nlength(EstLa[EstLa == 3.141593])\n```\n\n####Task 2.4b\n\n_We wish to discuss Lazzarini's design for estimating $\\pi$ and whether he did or did not base it on an estimation of $\\pi = 355/113$_\n\nIn terms of accuracy his approximation and ours are not significantly different. But his choice of $n,d,l$ and the spectacular result he got however seem a little bit suspicious. If he were to throw another needle then he would get amuch worse approximation whether or not the needle hit. It is likely that Lazzarini was are of the approximation of pi $\\hat{\\pi}$, which is also equal to the proportion $\\hat{\\pi}=\\frac{355}{113}$ that he got. Lazzarini could be aiming for this ratio when he conducted his experiment. By choosing $d=3$ and $l=2.5$, he would need a multiple 213 needles and a multiple 113 hits. This arrangement makes his design for estimating $\\pi$ biased, since one could simply abuse this design by throwing needles until the desired ratio occurs, and then report this result.\n\n##Task 3.\n\nTo solve the questions in this section, we use the data from the link:\n\n<https://raw.githubusercontent.com/haghish/ST516/master/data/gum.txt>\n\n```{r Read_gum}\n#Read data from the link\ngum <- read.table(\"https://raw.githubusercontent.com/haghish/ST516/master/data/gum.txt\")\n```\n\n###Task 3.1\n\nIn this task we created the funcion \"chi.probability(x,df=5,n=1000)\" in the package \"LemilExamST522\". Please see the function for more information, either by opening the file directly, or reading the help file.\n\n###Task 3.2\n\nIn this task we created the function \"chi.test(x,p)\" in the package \"LemilExamST522\". Please see the function for more information, either by opening the file directly, or reading the help file.\n\n###Task 3.3\n\nThe function \"chi.gof(x,p)\" takes 2 arguments, x and p. The function checks for x if it is a vector, matrix, list, and if it is in fact numbers. If x is not a vector or is not numeric, or is in fact either a matrix or a list, the function is stopped. This is because the function only accepts x as a vector.\n\nNext, it checks if p is missing, or in other words, that the user did not choose p. If this is the case, the function decides p as a vector of equal probabilities. It uses the length of x for this, which is why it needs to check if x is correctly chosen first. If p is not missing, the function checks that p is in fact also a vector of numbers, and not a matrix or list. Finally, it checks that p has the same length as x, because if it does not, it cannot be used.\n\nNext check that the sum of p is actually 1. But instead of stopping the function if this is not the case, we chose to only give a warning, because sometimes probabilities sum up to some value very close to 1, or the user might have some excuse to do this, and since the function can calculate it, the warning should be enough for the user to know what is wrong if the result is not acceptable.\n\nFinally, it checks whether x and p includes negative numbers, since negative probabilities does not make any since, and counting negative amounts of something in the sense x is supposed to fulfill does not make sense either. If either is negative, the function is stopped, since this is guaranteed to yield incorrect results.\n\n###Task 3.4\n\nThe null hypothesis in this case is the claim that all cards are equally likely to get in any shop. This is because it is reasonable to believe that one would have equal chance of getting each card if one purchases this gum from any shop. Moreover this is a claim which the company proposed and we yet have any evidence to say otherwise.\n\nThe alternative hypothesis in this case is, that it is not the case that all cards are equally likely to get in any shop.\n\nWe wish to test the company's claim using a Chi-Square Goodness of Fit test and the significant level of 0.01. The null hypothesis wil lbe rejected if the p-value is less than the chosen significant level.\n\nBased on the data provided by Gitte, under the assumption that the cards are equally likely to get, i.e. uniformly distributed, we calculate the Chi Squared test statistic using our function \"chi.test\" and evaluate the p-value using the function \"chi.probability\" for $n=10000$.\n\n```{r gum_Null_Hypothesis_test}\nChi <- chi.test(gum$Number)\nchi.probability(Chi,19, n = 10000)\n```\n\nThe estimated p-value is 0, which indicates strong evidence against the null hypothesis, that all cards are equally likely to get. We should expect about 15 of each card, however, what Gitte received is much different than what we expected. This extreme case is visualized using a histogram of the number of each card Gitte received, with a red line representing the expected number of each card.\n\n```{r Histogram_3_4}\nvariable <- c()\nfor (i in 1:length(gum$Number)){\n  variable <- c(variable,rep(i,gum$Number[i]))\n}\nhist(variable,breaks=seq(0.5,20.5,1), main=\"Histogram of card frequency\", xlab=\"Card Index Number\")\nabline(h=(301/20),col=\"red\")\n```\n\n###Task 3.5\n\n_We wish to compare our result to the result R gets from the functions it provides_\n\nBelow is the p-value calculated using the function \"Chisq.test\"\n\n```{r chisq_test}\nchisq.test(gum$Number)\n```\n\nAgain, we obtain a p-value, which is very close to zero and much less than the chosen significant level of 0.01, thus the null hypothesis is rejected. Gitte should consider the alternative hypothesis.\n\nWhy this difference is there between our result and the result from \"chisq.test\", we would assume is because \"chisq.test\" is either using a lot more random variables to estimate this, or uses a standardized method to get the result quickly, by simply using specific values that happen to estimate the result as closely as possible. This would both explain why the result is always the same when using \"chisq.test\", but varies when using \"chi.probability\", and that \"chisq.test\" is computed instantly. This is hard to conclude precisely, since \"chisq.test\" uses the function \"pchisq\", which calls functions that we do not have access to.\n\n###Task 3.6\n\n$P(X^2 > x)$ can be used to obtain the p-value, because in this case our choice of test statistic is of a Chi-Squared distribution and based on this statistic we want to determine, how likely it is to receive such a value for the test statistic, if the null hypothesis is true, i.e. how likely it is that Gitte would receive such observations by chance, if the company's claim were to be true.\n\n###Task 3.7\n\n_Now we wish to test if Gitte's friend's null hypothesis is true._\n\nGitte's friend proposed a list of probabilities for each card, and we wish to validate their proposal. This validation will be carried out using the Chi-Square Goodness of Fit test. The null hypothesis in this case is the proposal that Gitte's friend made against the alternative hypothesis that it is not the case that the cards have the probabilities that Gitte's friend proposed. The significant level is 0.01.\n\n```{r Gitte_Friend_Test}\nTestC <- chi.test(gum$Number,gum$expected)\nprint(pvalue <- chi.probability(TestC,df = 19,n=10000))\n```\n\nThe p-value is higher than the significant level of 0.01, thus, we do not reject Gitte's friend's proposal. Note that we cannot it even with the significant level of 0.05. We can in the same way as done in task 3.4 show this in a histogram\n\n```{r Histogram_3_7}\nhist(variable,breaks=seq(0.5,20.5,1), ylim=c(0,50), \n     main=\"Histogram of card frequency 2\", xlab=\"Card Index Number\")\nlines(seq(0.5,20.5,1),c(301*gum$expected[1],301*gum$expected), col=\"red\", type=\"S\")\n```\n\nWhere the red staircase shows the expected amount of cards based on Gitte's friends proposal. From this histogram, we can observe that most of the probabilities almost coincide with Gitte's results, but at least the five first probabilities are way off, which would explain the resulting p-value.\n\n#Exam Part B\n\n##Task 4.\n\nTo solve the questions in this section, we use the data from the link:\n<https://raw.githubusercontent.com/haghish/ST516/master/data/height.txt>\n\n```{r Read_height}\n#Read data from the link\nheight <- read.table(\"https://raw.githubusercontent.com/haghish/ST516/master/data/height.txt\")\n```\n\n###Task 4.1\n\nIn this task we created the function \"linear.model(formula)\" in the package \"LemilExamST522\". Please see the function for more information, either by opening the file directly, or reading the help file.\n\n###Task 4.2\n\nWe wish to generate $3$ variables, each of $1000$ observations, from the standard normal distribution, where the dependent variable has a correlation of $0.7$ and $0.3$ with the predictors and the predictors have a correlation of $0.0$ with each other.\n\nTo do this, we use Cholesky decomposition, wherein we use the following formula, where A is a matrix of the wanted correlations, and U is the matrix that we will use to obtain this. X, Y, Z are 3 variables. and where U is the transpose of L.\n\\[\nA = L \\cdot U, (X_i , Y_i , Z_i) \\cdot U = (F_i, G_i, H_i) \\Rightarrow cor(F,G) = 0.7, cor(F,H)=0.3, cor(G,H)=0.0\n\\]\nBased on our requirements, we have\n\\[\nA = \\begin{pmatrix}\n1 & 0.7 & 0.3 \\\\\n0.7 & 1 & 0.0 \\\\\n0.3 & 0.0 & 1\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0.7 & \\sqrt{1-0.7^2} & 0 \\\\\n0.3 & -{0.21 \\over \\sqrt{1-0.7^2}} & \\sqrt{0.91-({-0.21 \\over \\sqrt{1-0.7^2}})^2}\n\\end{pmatrix} \\cdot \\begin{pmatrix}\n1 & 0.7 & 0.3 \\\\\n0 & \\sqrt{1-0.7^2} & -{0.21 \\over \\sqrt{1-0.7^2}} \\\\\n0 & 0 & \\sqrt{0.91-({-0.21 \\over \\sqrt{1-0.7^2}})^2}\n\\end{pmatrix}\n\\]\n\nSo we have that\n\\[\nU = \\begin{pmatrix}\n1 & 0.7 & 0.3 \\\\\n0 & \\sqrt{1-0.7^2} & -{0.21 \\over \\sqrt{1-0.7^2}} \\\\\n0 & 0 & \\sqrt{0.91-({-0.21 \\over \\sqrt{1-0.7^2}})^2}\n\\end{pmatrix} \\approx \\begin{pmatrix}\n1 & 0.7 & 0.3 \\\\\n0 & 0.714143 & -0.294059 \\\\\n0 & 0 & 0.907485\n\\end{pmatrix}\n\\]\n\n```{r CorrelationMatrix_4_2}\nchoMatrix <- matrix(c(1,0,0,0.7,sqrt(0.51),0,0.3,-(0.21 / sqrt(0.51)),\n                      sqrt(0.91-(-0.21/sqrt(0.51))^2)),nrow=3,ncol=3)\nX <- rnorm(1000)\nY <- rnorm(1000)\nZ <- rnorm(1000)\nnorms <- matrix(c(X,Y,Z),nrow=1000,ncol=3)\nf <- matrix(rep(0,1000*3),nrow=1000,ncol=3)\nfor(i in 1:1000){\n  f[i,] <- norms[i,] %*% choMatrix\n}\nlist(\"Cor_F_G\" = cor(f[,1],f[,2]), \"Cor_F_H\" = cor(f[,1],f[,3]), \"Cor_G_H\" = cor(f[,2],f[,3]))\n```\n\nWe can observe that the correlations are approximately $0.7$, $0.3$ and $0.0$ respectively, as we expected.\n\n```{r Plots_And_Summaries_4_2}\npar(mfrow=c(1,3))\nplot(f[,2],f[,1], xlab=\"G\", ylab=\"F\")\nabline(lm(f[,1]~f[,2]),col=\"red\")\nplot(f[,3],f[,1], xlab=\"H\", ylab=\"F\")\nabline(lm(f[,1]~f[,3]),col=\"red\")\nplot(f[,3],f[,2], xlab=\"H\", ylab=\"G\")\nabline(lm(f[,2]~f[,3]),col=\"red\")\n\nFG <- linear.model(f[,1]~f[,2])\nFH <- linear.model(f[,1]~f[,3])\nGH <- linear.model(f[,2]~f[,3])\n\npar(mfrow=c(1,3))\nplot(f[,2],f[,1], xlab=\"G\", ylab=\"F\")\nabline(FG$coefficients[1,1],FG$coefficients[2,1],col=\"red\")\nplot(f[,3],f[,1], xlab=\"H\", ylab=\"F\")\nabline(FH$coefficients[1,1],FH$coefficients[2,1],col=\"red\")\nplot(f[,3],f[,2], xlab=\"H\", ylab=\"G\")\nabline(GH$coefficients[1,1],GH$coefficients[2,1],col=\"red\")\n```\n\nWe observe that the coefficents recieved fromt he regression models are equal to the specified correlations. This is however not a coincidence but a result from the fact that all of our variables are generated from a standard normal distribution and thus have variances of $1$. Now since we have\n\n\\[\n\\hat{\\beta}_i = \\frac{SX_iY}{SX_iX_i} = \\text{corr}(X_i,Y) \\cdot \\frac{\\sigma_Y}{\\sigma_{X_i}}\n\\]\n\nas $\\frac{\\sigma_Y}{\\sigma_{X_i}} = 1$, it then follows that\n\n\\[\n\\hat{\\beta}_{1,i} = \\text{corr}(X_i,Y)\n\\]\n\nOne cannot conclude that this is always the case as the ratio $\\frac{\\sigma_Y}{\\sigma_{X}}$ may differ from $1$. \n\nThe p-values for the coefficents and intercepts are also reported in the summaries. These p-values are obtained from a hypothesis testing using the t-statistic. This test is conducted for each of the intercepts and the coefficents under the assumption that they are zeros a.k.a. the null hypothesis. Thus these p-value indicates how likely it is that the predictor is of significance to our model (by significant we mean the relationship between this predictor and other variables). We will demonstrate this in the first model with the significant level of 0.05. The null hypothesis will be rejected if the p-value is less than the given significant level. \n\nIn the first model, we obtained the p-values of 0.18 and 0 for the intercept and the slope respectively. This means that we cannot reject the null hypothesis that the intercept is $0$. In other words we can not conclude whether or not the intercept is of significance to this model, however it is likely that it is. For the slope value, we obtained a p-value of 0. In this case the null hypothesis that it is zero is rejected i.e. the slope value is of significance to our model.\n\n###Task 4.3\n\n_We wish to explore if there are relations between, a fathers height, a mothers height, an individuals gender, or their parents socio-economic status, and the individuals height._\n\nTo get an overview of the relations that may or may not be, we start by testing the null hypotheses whether for each model the predictors has coefficients equal to zeros, that is to test whether or not there is a linear relationship between that predictor and the height. \n\nIt is likely that all the other predictors but SES have a linear relationship with the height of an individual. We will put this to test with the significant level of 0.05.\n\nAssuming there is a linear relation between SES and the height:\n\n```{r lm_Height_VS_SES}\nlinear.model(height$Height~height$SES)\n```\n\nWe observe that the p-value is in fact much greater than the chosen significant level 0.05, so it is very likely that there is no relation between SES and height of an individual. \n\nNext we explore the relationship between Father, Mother and Gender vs. height:\n\n```{r lm_Height_VS_Father_Mother_Gender}\nlinear.model(height$Height~height$Father)\nlinear.model(height$Height~height$Mother)\nlinear.model(height$Height~height$Gender)\n```\n\n__We__ see from each of these three summaries, that the p-value is much lower than the significant level 0.05, so we can reject the hypotheses that each the coefficients of each of the predictors are zero, and as such we conclude that, each of these are related to the height of an individual. \n\nWe propose the null hypothesis, that the height of an individual is related to their gender, as well as both of their parents' heights against the alternative hypothesis that at least one of the predictor are not of significant to the full model. That is we are interested in whether it is likely that the height can be predict by all of these predictors. \n\n```{r lm_Height_Vs_Father_Mother_Gender_Combined}\nlinear.model(height$Height~height$Gender+height$Father+height$Mother)\n```\n\nWe see from this summary, that in fact all three values together are related to the height of an individual, in such a way, that based on the given data, we can say that an individuals height is affected more by the height of the father, than the mother, but is affected by all 3 values nonetheless.\n\n##Task 5.\n\n_In this task, we work with densities, namely reconstructing functions that can even plot densities, just like the functions found in R._\n\n###Task 5.1\n\nIn this task we created the function \"dens.estimator(x,d,h,method=\"naive\")\" in the package \"LemilExamST522\". Please see the function for more information, either by opening the file directly, or reading the help file.\n\n###Task 5.2\n\nIn this task we created the function \"dens.plot(x,n=500,method=\"naive\",from,to)\" in the package \"LemilExamST522\". Please see the function for more information, either by opening the file directly, or reading the help file.\n\n###Task 5.3\n\n_We wish to look at the faithful data set._\n\n####Task 5.3a\n\nFirst we observe the results from the naive estimator.\n\n```{r Naive_Density}\ndens.estimator(faithful$eruptions,method=\"naive\")\n```\n\nNext, we observe the results from the Gaussian estimator.\n\n```{r Gaussian_Density}\ndens.estimator(faithful$eruptions,method=\"kernel\")\n```\n\nWe see that the results are simimilar, but in the naive case appears to vary much more based on little change on the point of observation.\n\n####Task 5.3b\n\nWe create the 2 plots using naive, and Gaussian estimators respectively.\n\n```{r plot_density}\npar(mfrow=c(1,2))\ndens.plot(faithful$eruptions,method=\"naive\")\ndens.plot(faithful$eruptions,method=\"kernel\")\n```\n\nHere we see that the naive plot appears very erratic and jumpy, while the Gaussian plot is much smoother. This relates to the report from the previous task.\n\n####Task 5.3c\n\nThe two formulas used to calculate bandwidth for the Naive method and Silverman's suggestion for the Gaussian method respectively, is\n\\[\nh = {R \\over 1 + log_2(n)} \\quad \\quad h=\\left( {4 \\hat{\\sigma}^5 \\over 3n} \\right)^{1 \\over 5} \\approx 1.06 \\hat{\\sigma} n^{-1/5}\n\\]\nWe see that for both formulas, the denominator is affected by the sample size $n$. For the naive case, $1+log_2(n)$, and for the Gaussian method, $n^{1 / 5}$. The difference here is that the naive case has a denominator that increases faster than the Gaussian case.\n\nINCOMPLETE INCOMPLETE INCOMPLETE\n\n###Task 6.\n\n_In this task, we wish to work with Markov Chains._\n\n####Task 6.1\n\nIn this task we created the function \"mc.discrete(p,k,n)\" in the package \"LemilExamST522\". Please see the function for more information, either by opening the file directly, or reading the help file.\n\n####Task 6.2\n\nBased on the diagram given, we get the following transition probability matrix\n\\[\n\\begin{pmatrix}\n0.2 & 0.7 & 0 & 0 & 0 & 0.1 \\\\\n0.3 & 0 & 0.7 & 0 & 0 & 0 \\\\\n0 & 0.5 & 0 & 0.5 & 0 & 0 \\\\\n0 & 0 & 0 & 0.9 & 0.1 & 0 \\\\\n0 & 0 & 0 & 0.25 & 0.5 & 0.25 \\\\\n0.4 & 0 & 0 & 0 & 0.4 & 0.2\n\\end{pmatrix}\n\\]\nNow, we wish to simulate this matrix using our function, with values $k=3$ and $n=10000$.\n\n```{r MC_Simulation}\np <- matrix(c(0.2,0.3,0,0,0,0.4,0.7,0,0.5,0,0,0,0,0.7,0,0,0,0,0,0,0.5,0.9,0.25,0,0,0,0,0.1,0.5,0.4,0.1,0,0,0,0.25,0.2),nrow=6,ncol=6)\nk <- 3\nn <- 10000\nprint(mc <- mc.discrete(p,k,n))\n```\n\nWe see that the probability of getting state 1 is `r mc[1]`, state 2 is `r mc[2]`, state 3 is `r mc[3]`, state 4 is `r mc[4]`, state 5 is `r mc[5]` and state 6 is `r mc[6]`.\n\nWe note that the probability of ending up at state 4 and 5 are much more likely than any of the other states. This also corresponds with the fact that these two states are the only ones in which the probability of not moving to a different state is greater, than the probability of moving to a different state. This since state 4 has $0.9$ chance of staying at state 4, and state 5 has $0.5$ chance of staying at state 5. While all other states are less likely to stay, some even having no chance of staying at their own state.\n\n",
    "created" : 1465292405130.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1204728938",
    "id" : "C91F9962",
    "lastKnownWriteTime" : 1465311146,
    "path" : "C:/Users/Emil/Dropbox/Universitet/ST522/FinalProjectCombined/ST522ExamEmilNguyen.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "type" : "r_markdown"
}